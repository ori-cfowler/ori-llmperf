
from llmperf.ray_llm_client import LLMClient
import ray
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import json
import ipdb
import os
import time
from typing import Any, Dict, Tuple

import ray
import requests

from llmperf.ray_llm_client import LLMClient
from llmperf.models import RequestConfig
from llmperf import common_metrics


@ray.remote
class Llama2LLMClient(LLMClient):
    """Client for Llama2 Chat Completions"""

    def llm_request(self, request_config: RequestConfig) -> Dict[str, Any]:
        """Make a single completion request to a LLM API

        Returns:
            Metrics about the performance charateristics of the request.
            The text generated by the request to the LLM API.
            The request_config used to make the request. This is mainly for logging purposes.

        """
        max_length = 2048

        access_token = os.environ.get("HF_ACCESS_TOKEN")
        model = "meta-llama/Llama-2-7b-chat-hf"
        tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)
        model = AutoModelForCausalLM.from_pretrained(model, token=access_token)
        llama_pipeline = pipeline("text-generation", model=model, torch_dtype=torch.float16, device_map="auto", tokenizer=tokenizer)

        prompt = request_config.prompt
        prompt, prompt_len = prompt

        time_to_next_token = []
        tokens_received = 0
        ttft = 0
        generated_text = ""
        error_msg = ""
        # output_throughput = 0
        # total_request_time = 0

        metrics = {}

        metrics[common_metrics.ERROR_CODE] = None
        metrics[common_metrics.ERROR_MSG] = ""


        most_recent_received_token_time = time.monotonic()

        try:
            input_ids = tokenizer.encode(prompt, return_tensors="pt")
            # Set model to evaluation mode
            model.eval()
            ttft_start_time = time.monotonic()
            with torch.no_grad():
                outputs = model(input_ids=input_ids)
                logits = outputs.logits[:, -1, :]
                next_token_id = torch.argmax(logits, dim=-1)

            ttft = time.monotonic() - ttft_start_time

            # Generate the full response
            start_time = time.monotonic()
            with torch.no_grad():
                outputs = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

            total_request_time = time.monotonic() - start_time
            tokens_received = outputs.shape[1]
            output_throughput = tokens_received / total_request_time

        except Exception as e:
            metrics[common_metrics.ERROR_MSG] = str(e)
            print(f"Warning Or Error: {e}")

        metrics["INTER_TOKEN_LAT"] = (total_request_time - ttft) / tokens_received
        metrics["TTFT"] = ttft
        metrics["E2E_LAT"] = total_request_time
        metrics["REQ_OUTPUT_THROUGHPUT"] = output_throughput
        metrics["NUM_TOTAL_TOKENS"] = tokens_received + prompt_len
        metrics["NUM_OUTPUT_TOKENS"] = tokens_received
        metrics["NUM_INPUT_TOKENS"] = prompt_len

        return metrics, generated_text, request_config

