
from llmperf.ray_llm_client import LLMClient
import ray
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import json
import os
import time
from typing import Any, Dict, Tuple

import ray
import requests

from llmperf.ray_llm_client import LLMClient
from llmperf.models import RequestConfig
from llmperf import common_metrics


@ray.remote
class Llama2LLMClient(LLMClient):
    """Client for Llama2 Chat Completions"""

    def llm_request(self, request_config: RequestConfig) -> Tuple[Metrics, str, RequestConfig]:
        """Make a single completion request to a LLM API

        Returns:
            Metrics about the performance charateristics of the request.
            The text generated by the request to the LLM API.
            The request_config used to make the request. This is mainly for logging purposes.

        """
        max_length = 2048
        # access_token = "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"
        access_token = os.environ.get("HF_ACCESS_TOKEN")
        model = "meta-llama/Llama-2-7b-chat-hf"
        tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)
        model = AutoModelForCausalLM.from_pretrained(model, token=access_token)
        llama_pipeline = pipeline("text-generation", model=model, torch_dtype=torch.float16, device_map="auto", tokenizer=tokenizer)

        prompt = request_config.prompt
        prompt, prompt_len = prompt

        time_to_next_token = []
        tokens_received = 0
        ttft = 0
        generated_text = ""
        error_msg = ""
        # output_throughput = 0
        # total_request_time = 0

        metrics = {}

        metrics[common_metrics.ERROR_CODE] = None
        metrics[common_metrics.ERROR_MSG] = ""


        most_recent_received_token_time = time.monotonic()

        try:
            input_ids = tokenizer.encode(prompt, return_tensors="pt")
            # Set model to evaluation mode
            model.eval()

            # Generate one token at a time
            for i in range(max_length):
                start_time = time.monotonic()
                with torch.no_grad():
                    outputs = model(input_ids=input_ids)
                    logits = outputs.logits[:, -1, :]
                    next_token_id = torch.argmax(logits, dim=-1)

                # Append the generated token to the input for next step
                input_ids = torch.cat([input_ids, next_token_id.unsqueeze(1)], dim=-1)

                # Decode the token and print
                next_token = tokenizer.decode(next_token_id)
                print(next_token, end="", flush=True)
                tokens_received += 1

                if not ttft:
                    ttft = time.monotonic() - start_time
                    time_to_next_token.append(ttft)
                else:
                    time_to_next_token.append(
                        time.monotonic() - most_recent_received_token_time
                    )
                most_recent_received_token_time = time.monotonic()
                generated_text += next_token
                # If the generated token is an end-of-text token, break
                if next_token == tokenizer.eos_token:
                    break
            total_request_time = time.monotonic() - start_time
            output_throughput = tokens_received / total_request_time
        except Exception as e:
            metrics[common_metrics.ERROR_MSG] = str(e)
            print(f"Warning Or Error: {e}")

        metrics[common_metrics.INTER_TOKEN_LAT] = sum(time_to_next_token)  # This should be same as metrics[common_metrics.E2E_LAT]. Leave it here for now
        metrics[common_metrics.TTFT] = ttft
        metrics[common_metrics.E2E_LAT] = total_request_time
        metrics[common_metrics.REQ_OUTPUT_THROUGHPUT] = output_throughput
        metrics[common_metrics.NUM_TOTAL_TOKENS] = tokens_received + prompt_len
        metrics[common_metrics.NUM_OUTPUT_TOKENS] = tokens_received
        metrics[common_metrics.NUM_INPUT_TOKENS] = prompt_len

        return metrics, generated_text, request_config

